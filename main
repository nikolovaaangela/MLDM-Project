import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score 
from sklearn.metrics import confusion_matrix, classification_report
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Input
from sklearn.metrics import precision_score, recall_score, f1_score

# Function to evaluate the model and print metrics
def evaluate_model(name, y_true, y_pred, results_dict):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    print(f"--- {name} ---")
    print(f"Accuracy:  {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall:    {recall:.2f}")
    print(f"F1-score:  {f1:.2f}")
    print("")

    results_dict[name] = {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1-score": f1
    }


# Load the dataset
df = pd.read_csv("heart.csv")

# Check for missing data
print(df.isnull().sum())
# There are no missing values, no need to encode categorical variables as they are already numerical  

# Split the dataset into features and target variable
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  #learns the mean and std and applies scaling
X_test_scaled = scaler.transform(X_test)        #applies the previously learned scaling â€” ensuring no data leakage from the test set

# Train the Logistic Regression model
results = {}
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Evaluate the model and predict on the test set
y_pred = model.predict(X_test_scaled)
evaluate_model("Logistic Regression", y_test, y_pred, results)


# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on test set and evaluate
y_pred = rf_model.predict(X_test)
evaluate_model("Random Forest", y_test, y_pred, results)


gbc = GradientBoostingClassifier(n_estimators=300,
                                 learning_rate=0.1,
                                 random_state=52,
                                 max_features='sqrt',
                                  max_depth=3) 

                                 
gbc.fit(X_train, y_train)

pred_y = gbc.predict(X_test)

evaluate_model("Gradient Boosting Classifier", y_test, pred_y, results)


classifier = Sequential()

classifier = Sequential()
classifier.add(Input(shape=(13,)))
classifier.add(Dense(units=12, kernel_initializer='uniform', activation='relu'))
classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))
classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
classifier.fit(X_train_scaled, y_train, batch_size=10, epochs=100, verbose=0)

# Evaluate ANN
loss, ann_accuracy = classifier.evaluate(X_test_scaled, y_test, verbose=0)
print("ANN Test Accuracy: {:.2f}%".format(ann_accuracy * 100))

# Predict and evaluate ANN
y_pred_ann = classifier.predict(X_test_scaled)
y_pred_ann = (y_pred_ann > 0.5)

evaluate_model("Neural Network", y_test, y_pred_ann, results)


results_df = pd.DataFrame(results).T
print("\nModel Performance Comparison:\n")
print(results_df.sort_values(by="F1-score", ascending=False))
