import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score 
from sklearn.metrics import confusion_matrix, classification_report
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Input
from sklearn.metrics import precision_score, recall_score, f1_score
import joblib
from keras.models import load_model
import numpy as np
import matplotlib.pyplot as plt


# Function to evaluate the model and print metrics
def evaluate_model(name, y_true, y_pred, results_dict):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    print(f"--- {name} ---")
    print(f"Accuracy:  {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall:    {recall:.2f}")
    print(f"F1-score:  {f1:.2f}")
    print("")

    results_dict[name] = {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1-score": f1
    }


# Load the dataset
df = pd.read_csv("heart.csv")

# Check for missing data
print(df.isnull().sum())
# There are no missing values, no need to encode categorical variables as they are already numerical  

# Split the dataset into features and target variable
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
#learns the mean and std and applies scaling
X_train_scaled = scaler.fit_transform(X_train)  
#applies the previously learned scaling to ensure that no data leaks from the test set
X_test_scaled = scaler.transform(X_test)     
# Save the scaler for later use
joblib.dump(scaler, "saved_models/scaler.pkl")


# Train the Logistic Regression model
results = {}
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Evaluate the model and predict on the test set
y_pred = model.predict(X_test_scaled)
evaluate_model("Logistic Regression", y_test, y_pred, results)

# Feature importance for Logistic Regression
coefficients = model.coef_[0]
feature_importance_lr = pd.Series(np.abs(coefficients), index=X.columns)
feature_importance_lr = feature_importance_lr.sort_values(ascending=False)

print("Logistic Regression Feature Influence (Absolute Coefficients):\n")
print(feature_importance_lr)

# Plot feature importances
plt.figure(figsize=(10, 6))
feature_importance_lr.plot(kind='bar', title='Feature Influence (Logistic Regression)')
plt.tight_layout()
plt.show()

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on test set and evaluate
y_pred = rf_model.predict(X_test)
evaluate_model("Random Forest", y_test, y_pred, results)


gbc = GradientBoostingClassifier(n_estimators=300,
                                 learning_rate=0.1,
                                 random_state=52,
                                 max_features='sqrt',
                                  max_depth=3) 

                                 
gbc.fit(X_train, y_train)

pred_y = gbc.predict(X_test)

# Feature importance for Gradient Boosting Classifier
importances = gbc.feature_importances_
feature_names = X.columns
indices = np.argsort(importances)[::-1]  

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.title("Feature Importance (Gradient Boosting)")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=45)
plt.tight_layout()
plt.show()

evaluate_model("Gradient Boosting Classifier", y_test, pred_y, results)


classifier = Sequential()

classifier = Sequential()
classifier.add(Input(shape=(13,)))
classifier.add(Dense(units=12, kernel_initializer='uniform', activation='relu'))
classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))
classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
classifier.fit(X_train_scaled, y_train, batch_size=10, epochs=100, verbose=0)

# Evaluate ANN
loss, ann_accuracy = classifier.evaluate(X_test_scaled, y_test, verbose=0)
print("ANN Test Accuracy: {:.2f}%".format(ann_accuracy * 100))

# Predict and evaluate ANN
y_pred_ann = classifier.predict(X_test_scaled)
y_pred_ann = (y_pred_ann > 0.5)

evaluate_model("Neural Network", y_test, y_pred_ann, results)

# Convert results to DataFrame  
results_df = pd.DataFrame(results).T
print("\nModel Performance Comparison:\n")
print(results_df.sort_values(by="F1-score", ascending=False))

import os

# Determine the best model
best_model_name = results_df["F1-score"].idxmax()
print(f"\n Best Model Based on F1-Score: {best_model_name}")

# Save the best model
model_dir = "saved_models"
os.makedirs(model_dir, exist_ok=True)

if best_model_name == "Logistic Regression":
    joblib.dump(model, f"{model_dir}/best_model.pkl")
    best_model_loaded = joblib.load(f"{model_dir}/best_model.pkl")

elif best_model_name == "Random Forest":
    joblib.dump(rf_model, f"{model_dir}/best_model.pkl")
    best_model_loaded = joblib.load(f"{model_dir}/best_model.pkl")

elif best_model_name == "Gradient Boosting Classifier":
    joblib.dump(gbc, f"{model_dir}/best_model.pkl")
    best_model_loaded = joblib.load(f"{model_dir}/best_model.pkl")

elif best_model_name == "Neural Network":
    classifier.save(f"{model_dir}/best_model.h5")
    best_model_loaded = load_model(f"{model_dir}/best_model.h5")

